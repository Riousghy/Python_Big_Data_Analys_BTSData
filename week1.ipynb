{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c509aec7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ignoring unknown arguments: ['--f=/Users/guohaoyang/Library/Jupyter/runtime/kernel-v32410e68be1e33ec94c578d6ef13aec1991e655e5.json']\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_1.zip\n",
      "[DL] progress: 1/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_2.zip\n",
      "[DL] progress: 2/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_3.zip\n",
      "[DL] progress: 3/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_4.zip\n",
      "[DL] progress: 4/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_9.zip\n",
      "[DL] progress: 5/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_10.zip\n",
      "[DL] progress: 6/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_11.zip\n",
      "[DL] progress: 7/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2024_12.zip\n",
      "[DL] progress: 8/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_1.zip\n",
      "[DL] progress: 9/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_2.zip\n",
      "[DL] progress: 10/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_3.zip\n",
      "[DL] progress: 11/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_4.zip\n",
      "[DL] progress: 12/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_9.zip\n",
      "[WARN] download failed 2025-9: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_9.zip\n",
      "[WARN] download failed 2025-9: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_9.zip\n",
      "[WARN] download failed 2025-9: HTTP Error 404: Not Found\n",
      "[DL] progress: 13/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_10.zip\n",
      "[WARN] download failed 2025-10: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_10.zip\n",
      "[WARN] download failed 2025-10: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_10.zip\n",
      "[WARN] download failed 2025-10: HTTP Error 404: Not Found\n",
      "[DL] progress: 14/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_11.zip\n",
      "[WARN] download failed 2025-11: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_11.zip\n",
      "[WARN] download failed 2025-11: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_11.zip\n",
      "[WARN] download failed 2025-11: HTTP Error 404: Not Found\n",
      "[DL] progress: 15/16\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_12.zip\n",
      "[WARN] download failed 2025-12: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_12.zip\n",
      "[WARN] download failed 2025-12: HTTP Error 404: Not Found\n",
      "[DL] https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2025_12.zip\n",
      "[WARN] download failed 2025-12: HTTP Error 404: Not Found\n",
      "[DL] progress: 16/16\n",
      "[DONE] downloaded: 12 ZIP files\n",
      "[UNZIP] Extracted CSV files: 12\n",
      "[READ] CSV files: 12\n",
      "[COLUMNS-SAMPLE] ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'FlightDate', 'Reporting_Airline', 'DOT_ID_Reporting_Airline', 'IATA_CODE_Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac'] ... total: 110\n",
      "[READ] 1/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_1.csv\n",
      "[READ] 2/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_10.csv\n",
      "[READ] 3/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_11.csv\n",
      "[READ] 4/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_12.csv\n",
      "[READ] 5/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_2.csv\n",
      "[READ] 6/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_3.csv\n",
      "[READ] 7/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_4.csv\n",
      "[READ] 8/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2024_9.csv\n",
      "[READ] 9/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2025_1.csv\n",
      "[READ] 10/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2025_2.csv\n",
      "[READ] 11/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2025_3.csv\n",
      "[READ] 12/12 On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2025_4.csv\n",
      "[CONCAT] shape: (6834001, 110)\n",
      "[PARQUET] Saved: /Users/guohaoyang/Desktop/vscworkspace/BTS/bts_on_time_data/parquet/bts_on_time_all.parquet\n",
      "[PARQUET] Partitioned parquet under: /Users/guohaoyang/Desktop/vscworkspace/BTS/bts_on_time_data/parquet/by_year_month\n",
      "\n",
      "[EDA] columns:\n",
      "['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'FlightDate', 'Reporting_Airline', 'DOT_ID_Reporting_Airline', 'IATA_CODE_Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac', 'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'Dest', 'DestCityName', 'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'CRSDepTime', 'DepTime', 'DepDelay', 'DepDelayMinutes', 'DepDel15', 'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrDel15', 'ArrivalDelayGroups', 'ArrTimeBlk', 'Cancelled', 'CancellationCode', 'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'Flights', 'Distance', 'DistanceGroup', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime', 'DivArrDelay', 'DivDistance', 'Div1Airport', 'Div1AirportID', 'Div1AirportSeqID', 'Div1WheelsOn', 'Div1TotalGTime', 'Div1LongestGTime', 'Div1WheelsOff', 'Div1TailNum', 'Div2Airport', 'Div2AirportID', 'Div2AirportSeqID', 'Div2WheelsOn', 'Div2TotalGTime', 'Div2LongestGTime', 'Div2WheelsOff', 'Div2TailNum', 'Div3Airport', 'Div3AirportID', 'Div3AirportSeqID', 'Div3WheelsOn', 'Div3TotalGTime', 'Div3LongestGTime', 'Div3WheelsOff', 'Div3TailNum', 'Div4Airport', 'Div4AirportID', 'Div4AirportSeqID', 'Div4WheelsOn', 'Div4TotalGTime', 'Div4LongestGTime', 'Div4WheelsOff', 'Div4TailNum', 'Div5Airport', 'Div5AirportID', 'Div5AirportSeqID', 'Div5WheelsOn', 'Div5TotalGTime', 'Div5LongestGTime', 'Div5WheelsOff', 'Div5TailNum', 'Unnamed: 109']\n",
      "\n",
      "[EDA] dtypes:\n",
      "Year                  int64\n",
      "Quarter               int64\n",
      "Month                 int64\n",
      "DayofMonth            int64\n",
      "DayOfWeek             int64\n",
      "                     ...   \n",
      "Div5TotalGTime      float64\n",
      "Div5LongestGTime    float64\n",
      "Div5WheelsOff       float64\n",
      "Div5TailNum         float64\n",
      "Unnamed: 109        float64\n",
      "Length: 110, dtype: object\n",
      "[EDA] missing_top25.csv saved\n",
      "[EDA] numeric_describe.csv saved\n",
      "[EDA] corr_subset.csv saved\n",
      "[EDA] sample_100k.parquet saved\n",
      "\n",
      "[ALL DONE] Week 1 complete: download → unzip → concat → Parquet → EDA\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Week 1 — BTS On-Time Performance: Data Acquisition & Initial EDA\n",
    "\n",
    "Pipeline:\n",
    "1) Download monthly ZIPs from BTS On-Time (1987–present)\n",
    "   https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_YYYY_M.zip\n",
    "2) Unzip CSVs\n",
    "3) Concatenate → write Parquet (full + year/month partitions)\n",
    "4) Initial EDA: columns/dtypes, missingness, numeric summary, correlations, plots (PNG)\n",
    "5) (Optional) Spark demo (--use-spark)\n",
    "\n",
    "Install:\n",
    "pip install numpy pandas pyarrow fastparquet matplotlib tqdm scikit-learn\n",
    "# Optional: pip install pyspark\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# CLI (robust to Jupyter args)\n",
    "# ---------------------------\n",
    "def parse_args(argv=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Week 1 — BTS On-Time: download → unzip → parquet → EDA\"\n",
    "    )\n",
    "    parser.add_argument(\"--years\", nargs=\"+\", type=int, default=None,\n",
    "                        help=\"Year list, e.g., 2024 2025\")\n",
    "    parser.add_argument(\"--months\", nargs=\"+\", type=int, default=None,\n",
    "                        help=\"Month list (1-12), can cross years\")\n",
    "    parser.add_argument(\"--auto-recent\", type=int, default=0,\n",
    "                        help=\"If >0, pull the most recent N months (overrides years/months)\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"./bts_on_time_data\",\n",
    "                        help=\"Output root directory\")\n",
    "    parser.add_argument(\"--limit-csv-rows-per-file\", type=int, default=None,\n",
    "                        help=\"Max rows per CSV to read (for memory-limited dry runs)\")\n",
    "    parser.add_argument(\"--sample-n\", type=int, default=100_000,\n",
    "                        help=\"Save a Parquet sample for fast iteration\")\n",
    "    parser.add_argument(\"--use-spark\", action=\"store_true\",\n",
    "                        help=\"Run a simple Spark aggregation demo (requires pyspark)\")\n",
    "    # Use parse_known_args so Jupyter’s --f/−f does not crash\n",
    "    args, unknown = parser.parse_known_args(argv)\n",
    "    if unknown:\n",
    "        print(f\"[INFO] Ignoring unknown arguments: {unknown}\")\n",
    "    return args\n",
    "\n",
    "def recent_year_month_pairs(n_recent: int):\n",
    "    assert n_recent > 0\n",
    "    now = datetime.utcnow()\n",
    "    y, m = now.year, now.month\n",
    "    pairs = []\n",
    "    for _ in range(n_recent):\n",
    "        pairs.append((y, m))\n",
    "        m -= 1\n",
    "        if m == 0:\n",
    "            y -= 1\n",
    "            m = 12\n",
    "    pairs.reverse()\n",
    "    years = {}\n",
    "    for y, m in pairs:\n",
    "        years.setdefault(y, []).append(m)\n",
    "    years = {y: sorted(ms) for y, ms in years.items()}\n",
    "    return years\n",
    "\n",
    "# ---------------------------\n",
    "# Download & unzip\n",
    "# ---------------------------\n",
    "BASE = \"https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{y}_{m}.zip\"\n",
    "\n",
    "def download_one(y: int, m: int, out: Path, retries: int = 2, sleep=2):\n",
    "    url = BASE.format(y=y, m=m)\n",
    "    if out.exists() and out.stat().st_size > 0:\n",
    "        return True\n",
    "    for k in range(retries + 1):\n",
    "        try:\n",
    "            print(f\"[DL] {url}\")\n",
    "            urllib.request.urlretrieve(url, out.as_posix())\n",
    "            if out.exists() and out.stat().st_size > 0:\n",
    "                return True\n",
    "        except (HTTPError, URLError) as e:\n",
    "            print(f\"[WARN] download failed {y}-{m}: {e}\")\n",
    "            if k < retries:\n",
    "                time.sleep(sleep * (k + 1))\n",
    "    return False\n",
    "\n",
    "def unzip_all(zip_paths, out_dir: Path):\n",
    "    n = 0\n",
    "    for zp in zip_paths:\n",
    "        try:\n",
    "            with zipfile.ZipFile(zp, 'r') as zf:\n",
    "                for name in zf.namelist():\n",
    "                    if name.lower().endswith(\".csv\"):\n",
    "                        zf.extract(name, out_dir)\n",
    "                        n += 1\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"[WARN] bad zip: {zp}\")\n",
    "    print(f\"[UNZIP] Extracted CSV files: {n}\")\n",
    "    return n\n",
    "\n",
    "# ---------------------------\n",
    "# Read → concat → Parquet\n",
    "# ---------------------------\n",
    "def infer_year_month_from_name(p: Path):\n",
    "    m = re.search(r\"(\\d{4})[_-](\\d{1,2})\", p.name)\n",
    "    if m:\n",
    "        return int(m.group(1)), int(m.group(2))\n",
    "    return None, None\n",
    "\n",
    "def read_concat_csvs(csv_files, per_file_nrows=None):\n",
    "    assert len(csv_files) > 0, \"No CSV files found. Check download/unzip steps.\"\n",
    "    print(f\"[READ] CSV files: {len(csv_files)}\")\n",
    "\n",
    "    sample = pd.read_csv(csv_files[0], nrows=1000, low_memory=False)\n",
    "    print(\"[COLUMNS-SAMPLE]\", list(sample.columns)[:20], f\"... total: {len(sample.columns)}\")\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(csv_files, 1):\n",
    "        print(f\"[READ] {i}/{len(csv_files)} {f.name}\")\n",
    "        df = pd.read_csv(f, nrows=per_file_nrows, low_memory=False)\n",
    "        if \"Year\" not in df.columns or \"Month\" not in df.columns:\n",
    "            y, m = infer_year_month_from_name(f)\n",
    "            if y is not None and \"Year\" not in df.columns:\n",
    "                df[\"Year\"] = y\n",
    "            if m is not None and \"Month\" not in df.columns:\n",
    "                df[\"Month\"] = m\n",
    "        dfs.append(df)\n",
    "\n",
    "    full = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"[CONCAT] shape:\", full.shape)\n",
    "    return full\n",
    "\n",
    "def write_parquet(full_df: pd.DataFrame, parquet_dir: Path):\n",
    "    parquet_dir.mkdir(parents=True, exist_ok=True)\n",
    "    all_path = parquet_dir / \"bts_on_time_all.parquet\"\n",
    "    try:\n",
    "        full_df.to_parquet(all_path, index=False)\n",
    "        print(\"[PARQUET] Saved:\", all_path.resolve())\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Failed to save all.parquet:\", e)\n",
    "\n",
    "    try:\n",
    "        part_cols = [c for c in (\"Year\", \"Month\") if c in full_df.columns]\n",
    "        if part_cols:\n",
    "            base = parquet_dir / \"by_year_month\"\n",
    "            for keys, g in full_df.groupby(part_cols):\n",
    "                keys = keys if isinstance(keys, tuple) else (keys,)\n",
    "                sub = base / \"/\".join(f\"{c}={v}\" for c, v in zip(part_cols, keys))\n",
    "                sub.mkdir(parents=True, exist_ok=True)\n",
    "                g.to_parquet(sub / \"part.parquet\", index=False)\n",
    "            print(\"[PARQUET] Partitioned parquet under:\", (parquet_dir / \"by_year_month\").resolve())\n",
    "        else:\n",
    "            print(\"[WARN] Year/Month not found; skip partition writing.\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Failed partition parquet:\", e)\n",
    "\n",
    "# ---------------------------\n",
    "# EDA\n",
    "# ---------------------------\n",
    "def eda_basic(df: pd.DataFrame, out_dir: Path, sample_n: int):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\n[EDA] columns:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    print(\"\\n[EDA] dtypes:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    miss = df.isna().sum().sort_values(ascending=False)\n",
    "    miss_pct = (miss / len(df)).round(4)\n",
    "    pd.DataFrame({\"missing\": miss, \"missing_pct\": miss_pct}).head(25).to_csv(out_dir / \"missing_top25.csv\")\n",
    "    print(\"[EDA] missing_top25.csv saved\")\n",
    "\n",
    "    num_desc = df.describe(include=\"number\").transpose().sort_values(\"count\", ascending=False)\n",
    "    num_desc.to_csv(out_dir / \"numeric_describe.csv\")\n",
    "    print(\"[EDA] numeric_describe.csv saved\")\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    corr = df[numeric_cols].corr(numeric_only=True)\n",
    "    keep = [c for c in numeric_cols if (\"Delay\" in c or \"Taxi\" in c or c == \"Distance\")]\n",
    "    corr.loc[[c for c in keep if c in corr.index], [c for c in keep if c in corr.columns]].round(3)\\\n",
    "        .to_csv(out_dir / \"corr_subset.csv\")\n",
    "    print(\"[EDA] corr_subset.csv saved\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if \"ArrDelay\" in df.columns:\n",
    "        plt.figure()\n",
    "        df[\"ArrDelay\"].dropna().clip(-60, 180).hist(bins=60)\n",
    "        plt.title(\"Histogram of ArrDelay (clipped [-60, 180])\")\n",
    "        plt.xlabel(\"ArrDelay (minutes)\"); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / \"hist_arrdelay.png\", dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    if \"DepDelay\" in df.columns:\n",
    "        plt.figure()\n",
    "        df[\"DepDelay\"].dropna().clip(-60, 180).hist(bins=60)\n",
    "        plt.title(\"Histogram of DepDelay (clipped [-60, 180])\")\n",
    "        plt.xlabel(\"DepDelay (minutes)\"); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / \"hist_depdelay.png\", dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "    carrier_col = \"Carrier\" if \"Carrier\" in df.columns else (\"UniqueCarrier\" if \"UniqueCarrier\" in df.columns else None)\n",
    "    if carrier_col and \"ArrDelay\" in df.columns:\n",
    "        small = df[[carrier_col, \"ArrDelay\"]].dropna()\n",
    "        if len(small) > 200_000:\n",
    "            small = small.sample(200_000, random_state=42)\n",
    "        order = small.groupby(carrier_col)[\"ArrDelay\"].median().sort_values().index[:15]\n",
    "        data = [small.loc[small[carrier_col]==c, \"ArrDelay\"].clip(-60, 180) for c in order]\n",
    "        plt.figure()\n",
    "        plt.boxplot(data, vert=True, labels=list(order), showfliers=False)\n",
    "        plt.title(\"ArrDelay by Carrier (top 15, clipped)\")\n",
    "        plt.xlabel(\"Carrier\"); plt.ylabel(\"ArrDelay (minutes)\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / \"box_arrdelay_by_carrier.png\", dpi=120)\n",
    "        plt.close()\n",
    "        print(\"[EDA] plots saved:\", out_dir)\n",
    "\n",
    "    sample_n = min(sample_n, len(df))\n",
    "    try:\n",
    "        df.sample(sample_n, random_state=42).to_parquet(out_dir / \"sample_100k.parquet\", index=False)\n",
    "        print(\"[EDA] sample_100k.parquet saved\")\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] save sample parquet failed:\", e)\n",
    "\n",
    "# ---------------------------\n",
    "# Spark (optional)\n",
    "# ---------------------------\n",
    "def spark_demo(parquet_dir: Path):\n",
    "    try:\n",
    "        import pyspark  # noqa\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark.sql import functions as F\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] pyspark not available:\", e)\n",
    "        return\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"bts-week1-eda\").getOrCreate()\n",
    "    base = (parquet_dir / \"by_year_month\").as_posix()\n",
    "    sdf = spark.read.parquet(base)\n",
    "    sdf.printSchema()\n",
    "    print(\"[SPARK] rows:\", sdf.count())\n",
    "\n",
    "    carrier_col = \"Carrier\" if \"Carrier\" in sdf.columns else (\"UniqueCarrier\" if \"UniqueCarrier\" in sdf.columns else None)\n",
    "    if carrier_col and \"ArrDelay\" in sdf.columns:\n",
    "        agg = (sdf.groupBy(carrier_col)\n",
    "               .agg(F.mean(\"ArrDelay\").alias(\"mean_arr_delay\"),\n",
    "                    F.expr(\"percentile_approx(ArrDelay, 0.5)\").alias(\"median_arr_delay\"),\n",
    "                    F.count(\"*\").alias(\"n\"))\n",
    "               .orderBy(F.col(\"n\").desc()))\n",
    "        agg.show(20, truncate=False)\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "\n",
    "    # Decide year/month selection\n",
    "    if args.auto_recent and args.auto_recent > 0:\n",
    "        year_to_months = recent_year_month_pairs(args.auto_recent)\n",
    "    else:\n",
    "        years = args.years if args.years else [2024, 2025]\n",
    "        months = args.months if args.months else [9, 10, 11, 12, 1, 2, 3, 4]\n",
    "        year_to_months = {}\n",
    "        for y in years:\n",
    "            year_to_months[y] = sorted([m for m in months if 1 <= m <= 12])\n",
    "\n",
    "    OUT = Path(args.outdir)\n",
    "    ZIP_DIR = OUT / \"zip\"\n",
    "    RAW_DIR = OUT / \"raw_csv\"\n",
    "    PARQUET_DIR = OUT / \"parquet\"\n",
    "    EDA_DIR = OUT / \"eda\"\n",
    "\n",
    "    for d in (OUT, ZIP_DIR, RAW_DIR, PARQUET_DIR, EDA_DIR):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Download\n",
    "    downloaded = []\n",
    "    total_targets = sum(len(ms) for ms in year_to_months.values())\n",
    "    done = 0\n",
    "    for y, months in sorted(year_to_months.items()):\n",
    "        for m in months:\n",
    "            done += 1\n",
    "            zpath = ZIP_DIR / f\"On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{y}_{m}.zip\"\n",
    "            if download_one(y, m, zpath):\n",
    "                downloaded.append(zpath)\n",
    "            print(f\"[DL] progress: {done}/{total_targets}\")\n",
    "    print(f\"[DONE] downloaded: {len(downloaded)} ZIP files\")\n",
    "\n",
    "    # 2) Unzip\n",
    "    unzip_all(downloaded, RAW_DIR)\n",
    "\n",
    "    # 3) Read & concat\n",
    "    csvs = sorted(RAW_DIR.glob(\"*.csv\"))\n",
    "    full = read_concat_csvs(csvs, per_file_nrows=args.limit_csv_rows_per_file)\n",
    "\n",
    "    # 4) Write Parquet\n",
    "    write_parquet(full, PARQUET_DIR)\n",
    "\n",
    "    # 5) EDA\n",
    "    eda_basic(full, EDA_DIR, sample_n=args.sample_n)\n",
    "\n",
    "    # 6) Spark (optional)\n",
    "    if args.use_spark:\n",
    "        spark_demo(PARQUET_DIR)\n",
    "\n",
    "    print(\"\\n[ALL DONE] Week 1 complete: download → unzip → concat → Parquet → EDA\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Allow direct runs AND `%run script.py ...` in notebooks\n",
    "    # by passing the real argv from the interpreter:\n",
    "    import sys\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ea664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
