{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78c8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Ignoring unknown arguments: ['--f=/Users/guohaoyang/Library/Jupyter/runtime/kernel-v381a5ca67ad28d9522eecfee939d36f10b14a52c3.json']\n",
      "[SPLIT] train=67485  val=24163  test=8352\n",
      "[CLEAN] Dropped 855 rows with NaN target 'ArrDelay'\n",
      "[CLEAN] Dropped 469 rows with NaN target 'ArrDelay'\n",
      "[CLEAN] Dropped 84 rows with NaN target 'ArrDelay'\n",
      "[REG] linreg  val={'MAE': 18635481.848606516, 'RMSE': 676305820.8841715, 'R2': -137667681320031.4}  test={'MAE': 9124250.80308588, 'RMSE': 479162648.70245594, 'R2': -83527695637906.27}  -> bts_on_time_data/eda/week3_reg_pred_linreg.csv\n",
      "[REG] rf  val={'MAE': 8.864394871114648, 'RMSE': 12.382560257019293, 'R2': 0.9538505374962416}  test={'MAE': 8.341302467343976, 'RMSE': 11.795224227671387, 'R2': 0.9493852624920154}  -> bts_on_time_data/eda/week3_reg_pred_rf.csv\n",
      "[CLF] logreg  val={'ROC_AUC': 0.5605660748907593, 'PR_AUC': 0.2307595299425911, 'best_thresh': 0.3439192131260163, 'best_F1': 0.33671343242082863, 'prec_at_best': 0.2152880573154124, 'rec_at_best': 0.7723024638912489, 'cm@0.5': {'threshold': 0.5, 'TP': 2505, 'FP': 8247, 'TN': 10739, 'FN': 2203}, 'cm@best': {'threshold': 0.3439192131260163, 'TP': 3636, 'FP': 13254, 'TN': 5732, 'FN': 1072}}  test={'ROC_AUC': 0.5822184622794265, 'PR_AUC': 0.2408386041438036, 'best_thresh': 0.3718390472880485, 'best_F1': 0.346835443037621, 'prec_at_best': 0.22508214676889376, 'rec_at_best': 0.7555147058823529, 'cm@0.5': {'threshold': 0.5, 'TP': 890, 'FP': 2751, 'TN': 3885, 'FN': 742}, 'cm@best': {'threshold': 0.3718390472880485, 'TP': 1233, 'FP': 4246, 'TN': 2390, 'FN': 399}}  -> bts_on_time_data/eda/week3_clf_prob_logreg.csv\n",
      "[CLF] PR curve saved -> bts_on_time_data/eda/week3_clf_pr_logreg.png\n",
      "[CLF] rf  val={'ROC_AUC': 0.565292979372808, 'PR_AUC': 0.23576994050121192, 'best_thresh': 0.04523530827541717, 'best_F1': 0.3380202699992933, 'prec_at_best': 0.20829424833374474, 'rec_at_best': 0.8961342395921835, 'cm@0.5': {'threshold': 0.5, 'TP': 382, 'FP': 1066, 'TN': 17920, 'FN': 4326}, 'cm@best': {'threshold': 0.04523530827541717, 'TP': 4219, 'FP': 16037, 'TN': 2949, 'FN': 489}}  test={'ROC_AUC': 0.5845926186930468, 'PR_AUC': 0.23927684228034252, 'best_thresh': 0.10647777982201612, 'best_F1': 0.3482332907616705, 'prec_at_best': 0.22659279778393351, 'rec_at_best': 0.7518382352941176, 'cm@0.5': {'threshold': 0.5, 'TP': 98, 'FP': 285, 'TN': 6351, 'FN': 1534}, 'cm@best': {'threshold': 0.10647777982201612, 'TP': 1227, 'FP': 4189, 'TN': 2447, 'FN': 405}}  -> bts_on_time_data/eda/week3_clf_prob_rf.csv\n",
      "[CLF] PR curve saved -> bts_on_time_data/eda/week3_clf_pr_rf.png\n",
      "[CLF] dt  val={'ROC_AUC': 0.5597756946248728, 'PR_AUC': 0.2331532715819382, 'best_thresh': 0.393416047034411, 'best_F1': 0.3343742001532047, 'prec_at_best': 0.22027382477911917, 'rec_at_best': 0.693712829226848, 'cm@0.5': {'threshold': 0.5, 'TP': 2606, 'FP': 8800, 'TN': 10186, 'FN': 2102}, 'cm@best': {'threshold': 0.393416047034411, 'TP': 3267, 'FP': 11577, 'TN': 7409, 'FN': 1441}}  test={'ROC_AUC': 0.5773444332901938, 'PR_AUC': 0.23812603302365612, 'best_thresh': 0.35596054475175604, 'best_F1': 0.34887302253919666, 'prec_at_best': 0.22609326800943566, 'rec_at_best': 0.7634803921568627, 'cm@0.5': {'threshold': 0.5, 'TP': 894, 'FP': 2823, 'TN': 3813, 'FN': 738}, 'cm@best': {'threshold': 0.35596054475175604, 'TP': 1249, 'FP': 4284, 'TN': 2352, 'FN': 383}}  -> bts_on_time_data/eda/week3_clf_prob_dt.csv\n",
      "[CLF] PR curve saved -> bts_on_time_data/eda/week3_clf_pr_dt.png\n",
      "[CLUSTER] best_k=3, silhouette=0.3352  -> bts_on_time_data/eda/week3_route_clusters.csv / bts_on_time_data/eda/week3_route_cluster_summary.csv\n",
      "[DONE] metrics json -> bts_on_time_data/eda/week3_metrics.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Week 3 â€” Baselines & Evaluation (BTS On-Time)\n",
    "- Temporal split: Train(2024), Val(2025-01..03), Test(2025-04)\n",
    "- Train-only profiles to avoid leakage (route_* / carrier_*)\n",
    "- Regression: ArrDelay  (MAE/RMSE/R2)\n",
    "- Classification: ArrDel15 (ROC-AUC/PR-AUC + threshold analysis)\n",
    "- Clustering: route-level KMeans with silhouette sweep (k=3..8)\n",
    "\"\"\"\n",
    "\n",
    "import argparse, json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score,\n",
    "                             roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, confusion_matrix)\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# CLI (robust to Jupyter args)\n",
    "# --------------------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Week 3 baselines on Week-2 features\")\n",
    "    p.add_argument(\"--root\", type=str, default=\"./bts_on_time_data\",\n",
    "                   help=\"Project root (contains eda/ and parquet/)\")\n",
    "    p.add_argument(\"--sample\", type=str, default=\"eda/sample_100k_week2_features.parquet\",\n",
    "                   help=\"Relative path to Week-2 features parquet\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    args, unknown = p.parse_known_args(argv)\n",
    "    if unknown:\n",
    "        print(f\"[INFO] Ignoring unknown arguments: {unknown}\")\n",
    "    return args\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Utilities\n",
    "# --------------------------\n",
    "PROFILE_COLS = [\n",
    "    \"route_med_arr_delay\",\"route_ontime_rate\",\"route_flights\",\n",
    "    \"carrier_med_arr_delay\",\"carrier_ontime_rate\",\"carrier_flights\"\n",
    "]\n",
    "\n",
    "def safe_carrier_col(df):\n",
    "    for c in [\"Reporting_Airline\", \"IATA_CODE_Reporting_Airline\", \"UniqueCarrier\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\"Carrier column not found\")\n",
    "\n",
    "def metrics_reg(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"RMSE\": float(mean_squared_error(y_true, y_pred, squared=False)),\n",
    "        \"R2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def choose_best_threshold(y_true, y_score):\n",
    "    ps, rs, ts = precision_recall_curve(y_true, y_score)[:3]\n",
    "    f1s = 2 * (ps * rs) / (ps + rs + 1e-12)\n",
    "    i = int(np.nanargmax(f1s))\n",
    "    thr = float(ts[i-1]) if i > 0 else 0.5\n",
    "    return thr, float(f1s[i]), float(ps[i]), float(rs[i])\n",
    "\n",
    "def confusion_at(y_true, y_prob, thr):\n",
    "    pred = (y_prob >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "    return {\"threshold\": float(thr), \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn)}\n",
    "\n",
    "def build_profiles_train(df_train, carrier_col):\n",
    "    r = (df_train.groupby([\"Origin\",\"Dest\"], dropna=False)\n",
    "         .agg(route_med_arr_delay=(\"ArrDelay\",\"median\"),\n",
    "              route_ontime_rate=(\"ArrDel15\", lambda s: 1 - np.nanmean(pd.to_numeric(s, errors=\"coerce\"))),\n",
    "              route_flights=(\"ArrDel15\",\"count\"))\n",
    "         .reset_index())\n",
    "    c = (df_train.groupby([carrier_col], dropna=False)\n",
    "         .agg(carrier_med_arr_delay=(\"ArrDelay\",\"median\"),\n",
    "              carrier_ontime_rate=(\"ArrDel15\", lambda s: 1 - np.nanmean(pd.to_numeric(s, errors=\"coerce\"))),\n",
    "              carrier_flights=(\"ArrDel15\",\"count\"))\n",
    "         .reset_index())\n",
    "    return r, c\n",
    "\n",
    "def attach_profiles(df, route_prof, carrier_prof, carrier_col):\n",
    "    df = df.drop(columns=[c for c in PROFILE_COLS if c in df.columns], errors=\"ignore\")\n",
    "    out = df.merge(route_prof, on=[\"Origin\",\"Dest\"], how=\"left\")\n",
    "    out = out.merge(carrier_prof, on=[carrier_col], how=\"left\")\n",
    "    return out\n",
    "\n",
    "def split_temporal(df):\n",
    "    train = df[df[\"Year\"] == 2024].copy()\n",
    "    val   = df[(df[\"Year\"] == 2025) & (df[\"Month\"].isin([1,2,3]))].copy()\n",
    "    test  = df[(df[\"Year\"] == 2025) & (df[\"Month\"] == 4)].copy()\n",
    "    return train, val, test\n",
    "\n",
    "def drop_na_target(df, ycol):\n",
    "    out = df[pd.notna(df[ycol])].copy()\n",
    "    dropped = len(df) - len(out)\n",
    "    if dropped:\n",
    "        print(f\"[CLEAN] Dropped {dropped} rows with NaN target '{ycol}'\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Feature spaces (leakage-safe)\n",
    "# --------------------------\n",
    "def setup_feature_spaces(df, carrier_col):\n",
    "    cat = [carrier_col, \"Origin\", \"Dest\", \"tod_bin\", \"season\"]\n",
    "    num = [\"Distance\", \"dow\", \"is_weekend\", \"quarter\",\n",
    "           \"route_med_arr_delay\",\"route_ontime_rate\",\"route_flights\",\n",
    "           \"carrier_med_arr_delay\",\"carrier_ontime_rate\",\"carrier_flights\",\n",
    "           \"CRSDepTime_min\"]  # scheduled signal\n",
    "\n",
    "    num_reg = list(num)\n",
    "    if \"DepDelay\" in df.columns: num_reg.append(\"DepDelay\")\n",
    "    if \"TaxiOut\"  in df.columns: num_reg.append(\"TaxiOut\")\n",
    "\n",
    "    num_clf = list(num)\n",
    "    return (num_reg, cat, \"ArrDelay\"), (num_clf, cat, \"ArrDel15\")\n",
    "\n",
    "\n",
    "def build_preprocessor(num_cols, cat_cols):\n",
    "    # Numeric: median impute -> scale\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    # OneHotEncoder API compatibility\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", num_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Models\n",
    "# --------------------------\n",
    "def regression_pipelines(prep):\n",
    "    return {\n",
    "        \"linreg\": Pipeline([(\"prep\", prep), (\"mdl\", LinearRegression())]),\n",
    "        \"rf\": Pipeline([(\"prep\", prep),\n",
    "                        (\"mdl\", RandomForestRegressor(\n",
    "                            n_estimators=250, random_state=42, n_jobs=-1))]),\n",
    "    }\n",
    "\n",
    "def classification_pipelines(prep):\n",
    "    # Includes Logistic Regression, Random Forest, and Decision Tree baselines\n",
    "    return {\n",
    "        \"logreg\": Pipeline([(\"prep\", prep),\n",
    "                            (\"mdl\", LogisticRegression(\n",
    "                                max_iter=1000, class_weight=\"balanced\"))]),\n",
    "        \"rf\": Pipeline([(\"prep\", prep),\n",
    "                        (\"mdl\", RandomForestClassifier(\n",
    "                            n_estimators=300, random_state=42, n_jobs=-1,\n",
    "                            class_weight=\"balanced\"))]),\n",
    "        \"dt\": Pipeline([(\"prep\", prep),\n",
    "                        (\"mdl\", DecisionTreeClassifier(\n",
    "                            max_depth=12,            # readable tree\n",
    "                            min_samples_leaf=100,    # regularization for imbalance\n",
    "                            class_weight=\"balanced\",\n",
    "                            random_state=42))]),\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Clustering (route-level)\n",
    "# --------------------------\n",
    "def route_clustering(df):\n",
    "    agg = (df.groupby([\"Origin\",\"Dest\"], dropna=False)\n",
    "             .agg(n=(\"ArrDel15\",\"count\"),\n",
    "                  ontime=(\"ArrDel15\", lambda s: 1 - np.nanmean(pd.to_numeric(s, errors=\"coerce\"))),\n",
    "                  med_arr=(\"ArrDelay\",\"median\"),\n",
    "                  mean_dist=(\"Distance\",\"mean\"))\n",
    "             .reset_index())\n",
    "    agg = agg[agg[\"n\"] >= 50].copy()\n",
    "    from sklearn.preprocessing import StandardScaler as _SS\n",
    "    X = _SS().fit_transform(agg[[\"ontime\",\"med_arr\",\"mean_dist\"]].values)\n",
    "\n",
    "    best_k, best_s, best_model = None, -1.0, None\n",
    "    for k in range(3, 9):\n",
    "        km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "        lab = km.fit_predict(X)\n",
    "        sil = silhouette_score(X, lab)\n",
    "        if sil > best_s:\n",
    "            best_k, best_s, best_model = k, sil, km\n",
    "\n",
    "    agg[\"cluster\"] = best_model.predict(X)\n",
    "    summary = (agg.groupby(\"cluster\")\n",
    "                 .agg(routes=(\"cluster\",\"count\"),\n",
    "                      avg_support=(\"n\",\"mean\"),\n",
    "                      med_ontime=(\"ontime\",\"median\"),\n",
    "                      med_arr_delay=(\"med_arr\",\"median\"),\n",
    "                      med_distance=(\"mean_dist\",\"median\"))\n",
    "                 .reset_index()\n",
    "                 .sort_values(\"routes\", ascending=False))\n",
    "    return agg, summary, {\"k\": best_k, \"silhouette\": float(best_s)}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# MAIN\n",
    "# --------------------------\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    ROOT = Path(args.root)\n",
    "    SAMPLE = ROOT / args.sample\n",
    "    if not SAMPLE.exists():\n",
    "        raise FileNotFoundError(f\"Features parquet not found: {SAMPLE}\")\n",
    "\n",
    "    df = pd.read_parquet(SAMPLE)\n",
    "    carrier_col = safe_carrier_col(df)\n",
    "\n",
    "    # sanity checks\n",
    "    req = [\"Year\",\"Month\",\"Origin\",\"Dest\",\"ArrDelay\",\"ArrDel15\",\n",
    "           \"Distance\",\"CRSDepTime_min\",\"dow\",\"is_weekend\",\"quarter\",\"season\",\"tod_bin\"]\n",
    "    miss = [c for c in req if c not in df.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"Missing required columns: {miss}\")\n",
    "\n",
    "    # temporal split\n",
    "    train, val, test = split_temporal(df)\n",
    "    print(f\"[SPLIT] train={len(train)}  val={len(val)}  test={len(test)}\")\n",
    "\n",
    "    # recompute profiles on TRAIN only (and drop any preexisting ones)\n",
    "    rprof, cprof = build_profiles_train(train, carrier_col)\n",
    "    train = attach_profiles(train, rprof, cprof, carrier_col)\n",
    "    val   = attach_profiles(val,   rprof, cprof, carrier_col)\n",
    "    test  = attach_profiles(test,  rprof, cprof, carrier_col)\n",
    "\n",
    "    # drop rows with NaN targets for regression\n",
    "    train = drop_na_target(train, \"ArrDelay\")\n",
    "    val   = drop_na_target(val,   \"ArrDelay\")\n",
    "    test  = drop_na_target(test,  \"ArrDelay\")\n",
    "\n",
    "    # feature spaces\n",
    "    (Xr_num, Xr_cat, y_reg), (Xc_num, Xc_cat, y_clf) = setup_feature_spaces(train, carrier_col)\n",
    "\n",
    "    # ================= REGRESSION =================\n",
    "    reg_prep = build_preprocessor(Xr_num, Xr_cat)\n",
    "    reg_models = regression_pipelines(reg_prep)\n",
    "    reg_results = {}\n",
    "\n",
    "    for name, pipe in reg_models.items():\n",
    "        pipe.fit(train[Xr_num + Xr_cat], train[y_reg])\n",
    "\n",
    "        def eval_split(s):\n",
    "            pred = pipe.predict(s[Xr_num + Xr_cat])\n",
    "            return metrics_reg(s[y_reg], pred), pred\n",
    "\n",
    "        m_val, _ = eval_split(val)\n",
    "        m_test, yhat_test = eval_split(test)\n",
    "        reg_results[name] = {\"val\": m_val, \"test\": m_test}\n",
    "\n",
    "        out_pred = ROOT / f\"eda/week3_reg_pred_{name}.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"FlightDate\": test.get(\"FlightDate\"),\n",
    "            \"Origin\": test[\"Origin\"], \"Dest\": test[\"Dest\"],\n",
    "            carrier_col: test[carrier_col],\n",
    "            \"y_true\": test[y_reg], \"y_pred\": yhat_test\n",
    "        }).to_csv(out_pred, index=False)\n",
    "        print(f\"[REG] {name}  val={m_val}  test={m_test}  -> {out_pred}\")\n",
    "\n",
    "    # ================ CLASSIFICATION ===============\n",
    "    for s in (train, val, test):\n",
    "        s[y_clf] = pd.to_numeric(s[y_clf], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    clf_prep = build_preprocessor(Xc_num, Xc_cat)\n",
    "    clf_models = classification_pipelines(clf_prep)\n",
    "    clf_results = {}\n",
    "\n",
    "    for name, pipe in clf_models.items():\n",
    "        pipe.fit(train[Xc_num + Xc_cat], train[y_clf])\n",
    "\n",
    "        def eval_split(s):\n",
    "            prob = pipe.predict_proba(s[Xc_num + Xc_cat])[:,1]\n",
    "            roc = float(roc_auc_score(s[y_clf], prob))\n",
    "            pr  = float(average_precision_score(s[y_clf], prob))\n",
    "            thr, f1b, pb, rb = choose_best_threshold(s[y_clf], prob)\n",
    "            cm_def  = confusion_at(s[y_clf], prob, 0.5)\n",
    "            cm_best = confusion_at(s[y_clf], prob, thr)\n",
    "            return {\"ROC_AUC\": roc, \"PR_AUC\": pr,\n",
    "                    \"best_thresh\": thr, \"best_F1\": f1b,\n",
    "                    \"prec_at_best\": pb, \"rec_at_best\": rb,\n",
    "                    \"cm@0.5\": cm_def, \"cm@best\": cm_best}, prob\n",
    "\n",
    "        m_val, _ = eval_split(val)\n",
    "        m_test, p_test = eval_split(test)\n",
    "        clf_results[name] = {\"val\": m_val, \"test\": m_test}\n",
    "\n",
    "        out_prob = ROOT / f\"eda/week3_clf_prob_{name}.csv\"\n",
    "        pd.DataFrame({\n",
    "            \"FlightDate\": test.get(\"FlightDate\"),\n",
    "            \"Origin\": test[\"Origin\"], \"Dest\": test[\"Dest\"],\n",
    "            carrier_col: test[carrier_col],\n",
    "            \"y_true\": test[y_clf], \"y_prob\": p_test\n",
    "        }).to_csv(out_prob, index=False)\n",
    "        print(f\"[CLF] {name}  val={m_val}  test={m_test}  -> {out_prob}\")\n",
    "\n",
    "        # PR curve\n",
    "        ps, rs, _ = precision_recall_curve(test[y_clf], p_test)\n",
    "        plt.figure()\n",
    "        plt.plot(rs, ps)\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR Curve (test) â€” {name}\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = ROOT / f\"eda/week3_clf_pr_{name}.png\"\n",
    "        plt.savefig(fig_path, dpi=120); plt.close()\n",
    "        print(f\"[CLF] PR curve saved -> {fig_path}\")\n",
    "\n",
    "    # ================= CLUSTERING ==================\n",
    "    routes, cluster_summary, best = route_clustering(df)\n",
    "    out_routes  = ROOT / \"eda/week3_route_clusters.csv\"\n",
    "    out_summary = ROOT / \"eda/week3_route_cluster_summary.csv\"\n",
    "    routes.to_csv(out_routes, index=False)\n",
    "    cluster_summary.to_csv(out_summary, index=False)\n",
    "    print(f\"[CLUSTER] best_k={best['k']}, silhouette={best['silhouette']:.4f}  -> {out_routes} / {out_summary}\")\n",
    "\n",
    "    # ================= METRICS JSON ===============\n",
    "    summary = {\"regression\": reg_results,\n",
    "               \"classification\": clf_results,\n",
    "               \"clustering\": best}\n",
    "    out_json = ROOT / \"eda/week3_metrics.json\"\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"[DONE] metrics json -> {out_json}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ae479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede9a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regression metrics ===\n",
      " Model Split            MAE            RMSE                    R2\n",
      "linreg  test  9,124,250.803 479,162,648.702  -83527695637906.2656\n",
      "    rf  test          8.341          11.795                0.9494\n",
      "linreg   val 18,635,481.849 676,305,820.884 -137667681320031.4062\n",
      "    rf   val          8.864          12.383                0.9539\n",
      "\n",
      "=== Classification metrics ===\n",
      " Model Split ROC_AUC PR_AUC BestThresh BestF1 Prec@Best Rec@Best\n",
      "    dt  test   0.577  0.238      0.356  0.349     0.226    0.763\n",
      "logreg  test   0.582  0.241      0.372  0.347     0.225    0.756\n",
      "    rf  test   0.585  0.239      0.106  0.348     0.227    0.752\n",
      "    dt   val   0.560  0.233      0.393  0.334     0.220    0.694\n",
      "logreg   val   0.561  0.231      0.344  0.337     0.215    0.772\n",
      "    rf   val   0.565  0.236      0.045  0.338     0.208    0.896\n",
      "\n",
      "=== Test confusion matrices ===\n",
      "\n",
      "[logreg] @0.5  -> {'threshold': 0.5, 'TP': 890, 'FP': 2751, 'TN': 3885, 'FN': 742}\n",
      "[logreg] @best -> {'threshold': 0.3718390472880485, 'TP': 1233, 'FP': 4246, 'TN': 2390, 'FN': 399}  (thr=0.372)\n",
      "\n",
      "[rf] @0.5  -> {'threshold': 0.5, 'TP': 98, 'FP': 285, 'TN': 6351, 'FN': 1534}\n",
      "[rf] @best -> {'threshold': 0.10647777982201612, 'TP': 1227, 'FP': 4189, 'TN': 2447, 'FN': 405}  (thr=0.106)\n",
      "\n",
      "[dt] @0.5  -> {'threshold': 0.5, 'TP': 894, 'FP': 2823, 'TN': 3813, 'FN': 738}\n",
      "[dt] @best -> {'threshold': 0.35596054475175604, 'TP': 1249, 'FP': 4284, 'TN': 2352, 'FN': 383}  (thr=0.356)\n",
      "\n",
      "=== Positive rate (ArrDel15=1) ===\n",
      "train=0.178  val=0.195  test=0.195\n",
      "\n",
      "=== Cluster summary ===\n",
      " cluster  routes  avg_support  med_ontime  med_arr_delay  med_distance\n",
      "       2     213    75.751174    0.838095           -8.0         591.0\n",
      "       0     131    74.038168    0.745455           -3.0         679.0\n",
      "       1      58    68.655172    0.816145           -9.0        1670.5\n",
      "\n",
      "=== Representative routes per cluster (top-5) ===\n",
      " cluster Origin Dest   n   ontime  med_arr  mean_dist\n",
      "       0    LAX  SFO 167 0.712575     -1.0      337.0\n",
      "       0    LAS  LAX 152 0.776316     -5.0      236.0\n",
      "       0    LAX  LAS 136 0.801471     -3.0      236.0\n",
      "       0    SFO  LAX 134 0.694030     -4.5      337.0\n",
      "       0    MCO  ATL 128 0.781250     -3.0      404.0\n",
      "       1    LAX  JFK 139 0.870504    -13.0     2475.0\n",
      "       1    JFK  LAX 135 0.888889    -17.0     2475.0\n",
      "       1    LAX  ORD 103 0.766990     -9.0     1744.0\n",
      "       1    ORD  LAX 103 0.805825     -9.0     1744.0\n",
      "       1    DFW  LGA 101 0.821782     -9.0     1389.0\n",
      "       2    OGG  HNL 162 0.895062     -5.0      100.0\n",
      "       2    HNL  OGG 156 0.923077     -2.0      100.0\n",
      "       2    LGA  ORD 156 0.769231    -11.5      733.0\n",
      "       2    DCA  BOS 147 0.857143    -12.0      399.0\n",
      "       2    DEN  PHX 145 0.806897     -6.0      602.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, pandas as pd\n",
    "ROOT = Path(\"./bts_on_time_data\")\n",
    "\n",
    "with open(ROOT/\"eda/week3_metrics.json\") as f:\n",
    "    M = json.load(f)\n",
    "\n",
    "# 1) Regression scoreboard\n",
    "rows = []\n",
    "for mdl, res in M[\"regression\"].items():\n",
    "    for split in (\"val\",\"test\"):\n",
    "        r = res[split]\n",
    "        rows.append({\"Model\": mdl, \"Split\": split,\n",
    "                     \"MAE\": r[\"MAE\"], \"RMSE\": r[\"RMSE\"], \"R2\": r[\"R2\"]})\n",
    "reg_df = pd.DataFrame(rows).sort_values([\"Split\",\"Model\"])\n",
    "print(\"\\n=== Regression metrics ===\")\n",
    "print(reg_df.to_string(index=False, formatters={\"MAE\":\"{:,.3f}\".format,\n",
    "                                                \"RMSE\":\"{:,.3f}\".format,\n",
    "                                                \"R2\":\"{:.4f}\".format}))\n",
    "\n",
    "# 2) Classification scoreboard\n",
    "rows = []\n",
    "for mdl, res in M[\"classification\"].items():\n",
    "    for split in (\"val\",\"test\"):\n",
    "        r = res[split]\n",
    "        rows.append({\"Model\": mdl, \"Split\": split,\n",
    "                     \"ROC_AUC\": r[\"ROC_AUC\"], \"PR_AUC\": r[\"PR_AUC\"],\n",
    "                     \"BestThresh\": r[\"best_thresh\"], \"BestF1\": r[\"best_F1\"],\n",
    "                     \"Prec@Best\": r[\"prec_at_best\"], \"Rec@Best\": r[\"rec_at_best\"]})\n",
    "clf_df = pd.DataFrame(rows).sort_values([\"Split\",\"Model\"])\n",
    "print(\"\\n=== Classification metrics ===\")\n",
    "print(clf_df.to_string(index=False, formatters={\"ROC_AUC\":\"{:.3f}\".format,\n",
    "                                                \"PR_AUC\":\"{:.3f}\".format,\n",
    "                                                \"BestThresh\":\"{:.3f}\".format,\n",
    "                                                \"BestF1\":\"{:.3f}\".format,\n",
    "                                                \"Prec@Best\":\"{:.3f}\".format,\n",
    "                                                \"Rec@Best\":\"{:.3f}\".format}))\n",
    "\n",
    "# 3) Confusion matrices @0.5 and @best (test only)\n",
    "print(\"\\n=== Test confusion matrices ===\")\n",
    "for mdl in (\"logreg\",\"rf\",\"dt\"):\n",
    "    r = M[\"classification\"][mdl][\"test\"]\n",
    "    print(f\"\\n[{mdl}] @0.5  -> {r['cm@0.5']}\")\n",
    "    print(f\"[{mdl}] @best -> {r['cm@best']}  (thr={r['best_thresh']:.3f})\")\n",
    "\n",
    "# 4) Positive rates\n",
    "feats = pd.read_parquet(ROOT/\"eda/sample_100k_week2_features.parquet\")\n",
    "pos = lambda df: (pd.to_numeric(df[\"ArrDel15\"], errors=\"coerce\")==1).mean()\n",
    "prt = pos(feats[feats[\"Year\"]==2024])\n",
    "prv = pos(feats[(feats[\"Year\"]==2025)&(feats[\"Month\"].isin([1,2,3]))])\n",
    "prs = pos(feats[(feats[\"Year\"]==2025)&(feats[\"Month\"]==4)])\n",
    "print(\"\\n=== Positive rate (ArrDel15=1) ===\")\n",
    "print(f\"train={prt:.3f}  val={prv:.3f}  test={prs:.3f}\")\n",
    "\n",
    "# 5) Clusters summary + representatives\n",
    "clusters = pd.read_csv(ROOT/\"eda/week3_route_clusters.csv\")\n",
    "summary  = pd.read_csv(ROOT/\"eda/week3_route_cluster_summary.csv\")\n",
    "print(\"\\n=== Cluster summary ===\")\n",
    "print(summary.to_string(index=False))\n",
    "reproutes = (clusters.sort_values([\"cluster\",\"n\"], ascending=[True,False])\n",
    "                     .groupby(\"cluster\").head(5)[[\"cluster\",\"Origin\",\"Dest\",\"n\",\"ontime\",\"med_arr\",\"mean_dist\"]])\n",
    "print(\"\\n=== Representative routes per cluster (top-5) ===\")\n",
    "print(reproutes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadb2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
